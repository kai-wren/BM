{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../zdrojaky')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import nig\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Sequential estimation of linear models, prediction\n",
    "\n",
    "\n",
    "Let us assume a random process $\\{Y_t|X_t; t=1,2,\\ldots\\}$ with **independent identically distributed (iid) realizations** $y_1, y_2,\\ldots$ determined by a **known observable variable** $X_t$, e.g., the **regressor**, if present. Our goal is to model this process using a suitable probabilistic **model** $f(y_t|x_t, \\theta)$, where $\\theta$ is an **unknown parameter**. Its reliable estimation is crucial. Furthermore, let our task be an online, i.e., **sequential**, estimation.\n",
    "\n",
    "There are multiple ways towards online modelling, for instance:\n",
    "- periodic estimation on **data window**, aggregating all data from the beginning time $t=1$ up to the present time. As the amount of data increases, there are high demands on memory and computational performance.\n",
    "- periodic estimation on **floating data window**. This approach is one of the most popular, as it quite reasonably exploits only recent data, e.g., the last 100 measurements.\n",
    "- **fully sequential estimation**, where the previously available information is only updated by the most recent data. We saw a demonstration of this in the previous lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation example\n",
    "\n",
    "Say, that we measure the altitude $y_t$ of a climbing object by an *imprecise* radar instrument. We denote the speed at the moment of the first detection as $v_0$ and assume a constant acceleration $a$. The altitude of the first detection is set $y_0 = 0$. \n",
    "\n",
    "**The goals are:**\n",
    "- online sequential **prediction of the altitude** at the next time instant, $y_{t+1}$,\n",
    "- online **estimation of $a$ and $v_0$**,\n",
    "- online **estimation of the noise variance**. Note that this noise is due to the terrain, weather conditions, intrinsic noise in the instrument etc.\n",
    "\n",
    "From physics we know that the height is given by $v_0 t + \\frac{1}{2}a t^2$. We add a noise variable $\\varepsilon$, and the resulting model reads\n",
    "\n",
    "$$\n",
    "    y_t = v_0 t + \\frac{1}{2} a t^2 + \\varepsilon_t.\n",
    "$$\n",
    "\n",
    "Let us look at the evolution of the first measurements and the online predictions. The blue line depicts true measurements, crosses stand for prediction (red are current, green past).\n",
    "![Výška a predikce](img/l2-regrese-anim.gif)\n",
    "\n",
    "And for the whole data set after 3, 10, 20, 30, 50 and 80 measurements:\n",
    "![Výška a predikce](img/l2-predikce.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now shortly immerse into the theory and then return back to this example...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian estimation\n",
    "\n",
    "Denote by $y_t$ the random variable observed at discrete time instants $t=0, 1,2,\\ldots$, and by $y_{0:t-1} = [y_0, y_1, \\ldots, y_{t-1}]$. Assume that the $y_t$ are iid. Furthermore, let $y_t$ be determined by a known observed variable $x_t$ (e.g., the regressor), and by a constant parameter $\\theta$. We denote $x_{0:t-1} = [x_0, x_1, \\ldots, x_{t-1}]$. \n",
    "\n",
    "Now assume that there is a prior distribution (density) $\\pi(\\theta|x_0, y_0)$ expressing our prior knowledge. $x_0$ and $y_0$ can be seen as pseudo-data.\n",
    "\n",
    "> **Bayes' theorem**\n",
    ">\n",
    "> Let $f(y_t|x_t, \\theta)$ be a pdf of $y_t|x_t,\\theta$. Let $\\pi(\\theta|y_{0:t-1}, x_{0:t-1})$ be the prior density for $\\theta$. The one-step Bayesian update yields a posterior density of the form\n",
    ">\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\pi(\\theta|y_{0:t}, x_{0,t}) \n",
    "&= \n",
    "\\frac\n",
    "{f(y_t|x_t, \\theta) \\pi(\\theta|x_{0:t-1}, y_{0:t-1})}\n",
    "{\\int f(y_t|x_t, \\theta) \\pi(\\theta|x_{0:t-1}, y_{0:t-1})d\\theta} \\\\\n",
    "&=\n",
    "\\frac\n",
    "{f(y_t|x_t, \\theta) \\pi(\\theta|x_{0:t-1}, y_{0:t-1})}\n",
    "{f(y_t|x_t)} \\\\\n",
    "&\\propto\n",
    "f(y_t|x_t, \\theta) \\pi(\\theta|x_{0:t-1}, y_{0:t-1}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Recall that the last row expresses the Bayesian update in terms of proportionality, i.e., without the normalizing density in the denominator. (How to calculate it?)\n",
    "\n",
    "\n",
    "Now we are interested in the sequential Bayesian update. It is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\pi(\\theta|y_{0:1}, x_{0:1})\n",
    "&\\propto\n",
    "f(y_1|x_1, \\theta) \\pi(\\theta|x_{0}, y_{0}) \\\\\n",
    "\\pi(\\theta|y_{0:2}, x_{0:2})\n",
    "&\\propto\n",
    "f(y_2|x_2, \\theta) \\pi(\\theta|x_{0:1}, y_{0:1}) \\\\\n",
    "&\\propto \\pi(\\theta|x_{0}, y_{0}) f(y_1|x_1, \\theta) f(y_2|x_2, \\theta) \\\\\n",
    "&\\vdots \\\\\n",
    "\\pi(\\theta|y_{0:t}, x_{0:t})\n",
    "&\\propto\n",
    "\\pi(\\theta|y_{0}, x_{0})\n",
    "\\prod_{\\tau = 1}^{t}\n",
    "f(y_{\\tau}|x_{\\tau}, \\theta) \\\\\n",
    "&\\propto\n",
    "\\pi(\\theta|y_{0:\\tau-1}, x_{0:\\tau-1})\n",
    "\\prod_{\\tau = \\widetilde{\\tau}}^{t}\n",
    "f(y_{\\widetilde\\tau}|x_{\\widetilde\\tau}, \\theta),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "i.e., we either may multiply the models and subsequently update the initial prior, or equivalently update at each time step by the most recent data. **Conclusion: the sequential one-by-one update is equivalent to the update by a batch of data.**\n",
    "\n",
    "\n",
    "** Think abouts... **\n",
    "- How to calculate the normalization term in the Bayes' theorem?\n",
    "- Think about the posterior distribution - which properties does it have? For instance, if we take any model and any prior, can we make direct conclusions about the mean value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential estimation\n",
    "\n",
    "We saw that the sequential Bayesian update is - theoretically - a simple task. We use the prior distribution, update it by new data, obtain the posterior distribution, which we reuse as the prior for the next time step:\n",
    "\n",
    "$$\n",
    "\\pi(\\theta|x_{0}, y_{0}) \\xrightarrow[\\text{Bayes}]{x_1, y_1}\n",
    "\\pi(\\theta|x_{0:1}, y_{0:1}) \\xrightarrow[\\text{Bayes}]{x_2, y_2}\n",
    "\\pi(\\theta|x_{0:2}, y_{0:2}) \\rightarrow\n",
    "\\cdots \\xrightarrow[\\text{Bayes}]{x_t, y_t}\n",
    "\\pi(\\theta|x_{0:t}, y_{0:t}) \\rightarrow\n",
    "\\cdots\n",
    "$$\n",
    "\n",
    "Furthermore recall that the point estimate of $\\theta$ may serve\n",
    "- the expected value $\\mathbb{E}[\\theta]$, more precisely $\\mathbb{E}[\\theta|x_{0:t}, y_{0:t}]$, or \n",
    "- the mode - maximum aposteriori (MAP) estimate, or \n",
    "- the median.\n",
    "\n",
    "The uncertainty in the Bayesian estimate is represented mostly by the variance of the prior/posterior distribution, $\\text{var}\\theta$.\n",
    "\n",
    "**\n",
    "Although the general equations look quite simple, the most fundamental problem of Bayesian modelling is the derivation of the posterior distribution and its properties. This is particularly true for sequential modelling. If the posterior distribution after the first update is not any \"standard\" distribution, the subsequent update makes it yet more complicated.\n",
    "**\n",
    "\n",
    "**\n",
    "Fortunately, there are cases where the Bayes' theorem yields analytically tractable (and \"standard\") distribution. For this purpose, let us introduce the exponential family of distributions and conjugate prior distributions.\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition (exponential family of distributions, EF)**\n",
    ">\n",
    "> Assume a random variable $y$ conditioned by $x$ and a parameter $\\theta$. The EF is a **class of distributions** with pdfs of the form\n",
    ">\n",
    "> $$\n",
    "f(y|x, \\theta) = h(y, x) g(\\theta) \\exp \\left[ \\eta^{\\intercal} T(y,x) \\right],\n",
    "$$\n",
    ">\n",
    "> where $\\eta \\equiv \\eta(\\theta)$ is the **natural parameter**, $T(y,x)$ is the **sufficient statistic**, $h(y,x)$ is a **known function**, and $g(\\theta)$ is a **normalization function**. If $\\eta(\\theta)=\\theta$ the class is canonical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition (conjugate prior distribution)**\n",
    ">\n",
    "> Let $y|x, \\theta$ have a distribution from the EF. We say that the **prior distribution** for $\\theta$ with **hyperparameters** $\\xi$ and $\\nu$ is **conjugate to the model**, if its pdf has the form\n",
    ">\n",
    ">$$\n",
    "        \\pi(\\theta) = q(\\xi, \\nu) g(\\theta)^{\\nu} \\exp \\left[ \\eta^{\\intercal} \\xi \\right],\n",
    "$$\n",
    ">\n",
    ">where $\\xi$ has the same size as $T(y,x)$, $\\nu\\in\\mathbb{R}^{+}$, and $q(\\xi,\\nu)$ is a known function. The function $g(\\theta)$ is the same as in the definition of EF for the model of $y|x, \\theta$.\n",
    "\n",
    "Recall, that hyperparameters are parameters of the prior - to avoid with the model parameter $\\theta$. If the prior has its own prior, its hyperparameters are sometimes called as hyper-hyperparameters, but it's a bit...strange :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of conjugate priors\n",
    "\n",
    "Although we did not rewrite the binomial distribution into the EF form and the beta distribution to the compatible conjugate form, we saw, that the conjugacy was fulfilled. Of course, under conjugacy, it is possible to evaluate posteriors without any rewriting, but...we will see shortly why Kamil likes it :)\n",
    "\n",
    "| Model | Use | Conjugate prior |\n",
    "|:---|:---:|:---|\n",
    "|Normal with known variance | Tracking, physics... :-) | Normal |\n",
    "|Normal with unknown variance | Everywhere :-) | Normal inverse-gamma |\n",
    "|Bernoulli | Success-Failure (coin, reliability) | Beta |\n",
    "|Binomial |  Success-Failure (coin, reliability) | Beta |\n",
    "|Poisson | Traffic, particle physics | Gamma |\n",
    "|Multinomial | Classification | Dirichlet |\n",
    "\n",
    "See [wikipedia](https://en.wikipedia.org/wiki/Conjugate_prior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian estimation with conjugate prior\n",
    "If we assume conjugate prior with hyperparameters $\\xi_{t-1}$ a $\\nu_{t-1}$, the Bayes update\n",
    "\n",
    "$$\n",
    "\\pi(\\theta|y_{0:t}, x_{0,t}) \n",
    "\\propto\n",
    "f(y_t|x_t, \\theta) \\pi(\\theta|x_{0:t-1}, y_{0:t-1})\n",
    "$$\n",
    "\n",
    "is a trivial summation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\xi_{t} &= \\xi_{t-1} + T(y_{t},x_{t}), \\\\\n",
    "    \\nu_{t} &= \\nu_{t-1} + 1.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "With multiple data,\n",
    "\n",
    "$$\n",
    "\\pi(\\theta|y_{0:1}, x_{0:1})\n",
    "\\propto\n",
    "\\pi(\\theta|y_{0:\\tau-1}, x_{0:\\tau-1})\n",
    "\\prod_{\\tau = \\widetilde{\\tau}}^{t}\n",
    "f(y_{\\widetilde\\tau}|x_{\\widetilde\\tau}, \\theta),\n",
    "$$\n",
    "\n",
    "it is again a summation!\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\xi_{t} &= \\xi_{\\tau-1} + \\sum_{\\widetilde{\\tau}=\\tau}^{t} T(y_{\\widetilde{\\tau}},x_{\\widetilde{\\tau}}),\\\\\n",
    "    \\nu_{t} &= \\nu_{\\tau-1} + t - \\tau+1.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Conclusion: Under conjugacy, the Bayes' theorem is simply a sum of the sufficient statistics and the hyperparameter $\\xi_{t-1}$ and an incrementation of $\\nu_{t-1}$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Ping\n",
    "The availability of an internet server is tested by a *ping* message (ECHO REQUEST). Say that we send it each 500ms and expect the reply (ECHO REPLY) to arrive within 50ms. We describe the server availability as the probability $p \\in [0, 1]$ of receiving a reply. Denote this by $X=1$ (\"success\").\n",
    "\n",
    "#### Model\n",
    "Since the modelled variable $X\\in\\{0, 1\\}$ is binary and has a probability $p$, a suitable distribution is the Bernoulli distribution with a pmf\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(x_t|p) &= p^x (1-p)^{1-x} \\\\\n",
    "&= \\exp\\{ \\ln [p^x_t \\cdot (1-p)^{1-x_t}] \\} \\\\\n",
    "&= \\exp\\{x_t \\ln p + (1-x_t) \\ln(1-p)\\} \\\\\n",
    "&= \\exp \n",
    "\\left\\{\n",
    "\\begin{bmatrix}\n",
    "\\ln p \\\\\n",
    "\\ln (1-p)\n",
    "\\end{bmatrix}^\\intercal\n",
    "\\begin{bmatrix}\n",
    "x_t \\\\\n",
    "1-x_t\n",
    "\\end{bmatrix}\n",
    "\\right\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "hence $h(x) = 1$, $g(\\theta)=g(\\pi) = 1$. Note that the EF form is neither unique nor nice here, but it is practical. We will see why.\n",
    "\n",
    "#### Prior for $p$\n",
    "We also know that the probability $p$ can be modelled by the beta distribution with hyperparameters $a_{t-1}, b_{t-1}>0$. Its pdf is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\pi(p|a_{t-1}, b_{t-1})\n",
    "&= \\frac{1}{B(a_{t-1}, b_{t-1})} p^{1-a_{t-1}} (1-p)^{1-b_{t-1}} \\\\\n",
    "&= \\frac{1}{B(a_{t-1}, b_{t-1})} \n",
    "\\exp \n",
    "\\left\\{\n",
    "\\begin{bmatrix}\n",
    "\\ln p \\\\\n",
    "\\ln (1-p)\n",
    "\\end{bmatrix}^\\intercal\n",
    "\\begin{bmatrix}\n",
    "a_{t-1} - 1 \\\\\n",
    "b_{t-1} - 1\n",
    "\\end{bmatrix}\n",
    "\\right\\}\n",
    "\\end{aligned}.\n",
    "$$\n",
    "\n",
    "Thus we see that the prior is conjugate with $q(\\cdot) = B(\\cdot)$, hyperparameter $\\nu_{t-1}$ as an exponent of $g(\\theta) = 1$ (thus we may ignore it), the vector $\\xi_{t-1}$ is the second vector in the exponent.\n",
    "\n",
    "#### Sequential Bayesian update\n",
    "$$\n",
    "\\xi_t = \\xi_{t-1} + T(x_t) \\qquad \\Rightarrow \\qquad a_t = a_{t-1} + x_t, \\qquad b_t = b_{t-1} + (1-x_t).\n",
    "$$\n",
    "\n",
    "#### Posterior estimates\n",
    "$$\n",
    "\\hat{p} = \\frac{a_t}{a_t + b_t}, \\qquad var(\\hat{p}) = \\frac{a_t b_t}{(a_t + b_t)^2 (a_t + b_t + 1)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression model\n",
    "\n",
    "We should already know the linear regression model\n",
    "\n",
    "$$\n",
    "y_t = \\beta^\\intercal x_t + \\varepsilon_t, \\qquad t=1,2,\\ldots,\n",
    "$$\n",
    "\n",
    "where $y_t$ is an observed scalar variable, $x_t$ is a known regression vector, and $\\beta$ is a vector of regression coefficients of the same size. $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ is and iid noise. \n",
    "According to the form of the model, there are several model types, e.g., a straight line, quadratic function, higher-order polynomial etc.\n",
    "\n",
    "From statistics we should know that the maximum likelihood estimate $\\beta = (X^\\intercal X)^{-1} X^\\intercal y$, where $X$ a $y$ are matrices whose rows are regressors $x_t$, and $y$ is a column vector of $y_t$s, respectively. \n",
    "\n",
    "![Regrese](img/l2-linmodely.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian linear regression\n",
    "\n",
    "### Model\n",
    "Assume an observed $y_t$ determined by the regressor $x_t\\in\\mathbb{R}^p$ and a vector of regression coefficients $\\beta\\in\\mathbb{R}^p$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_t &= \\beta^\\intercal x_t + \\varepsilon_t, \\\\\n",
    "\\varepsilon_t &\\sim \\mathcal{N}(0, \\sigma^2) \\qquad \\text{iid}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "> Recall our altitude tracking example:\n",
    "> ![Regrese](img/l2-sigmapas.jpg)\n",
    "> We already know that the corresponding model is\n",
    "$$\n",
    "y_t = v_0 t + \\frac{1}{2} a t^2 + \\varepsilon_t =\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "v_0 \\\\\n",
    "a\n",
    "\\end{bmatrix}^\\intercal\n",
    "}_{\\beta^\\intercal}\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "t \\\\\n",
    "\\frac{1}{2}t^2\n",
    "\\end{bmatrix}\n",
    "}_{x_t}\n",
    "+ \\varepsilon_t\n",
    "$$\n",
    "\n",
    "> **Think abouts...**\n",
    "- Modify the model for nonzero initial altitude.\n",
    "- How to predict the altitude for a preset time (say $t=100$) if we know $a$ and $v_0$?\n",
    "- That is: what do we need to know in order to make predictions?\n",
    "\n",
    "---\n",
    "\n",
    "**Task: estimation of unknown constant $\\beta$ and $\\sigma^2$. And naturally prediction for given $x'$.**\n",
    "\n",
    "Since the measurement noise is normal, the model is normal too, $y_t\\sim\\mathcal{N}(\\beta^\\intercal x_t, \\sigma^2)$. Its pdf is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    f(y_{t}|x_{t}, \\beta, \\sigma^{2}) \n",
    "    &= \\frac{(\\sigma^{2})^{-\\frac{1}{2}}}{\\sqrt{2\\pi}}\n",
    "       \\exp\n",
    "       \\left\\{ \n",
    "           -\\frac{1}{2\\sigma^{2}} (y_{t} - \\beta^{\\intercal}x_{t})^{2} \n",
    "       \\right\\} \\notag \\\\\n",
    "    &= \\frac{(\\sigma^{2})^{-\\frac{1}{2}}}{\\sqrt{2\\pi}}\n",
    "       \\exp\n",
    "       \\Bigg\\{ \n",
    "           \\text{Tr}\n",
    "           \\bigg( \n",
    "               \\underbrace{\n",
    "                   -\\frac{1}{2\\sigma^{2}}\n",
    "                   \\begin{bmatrix}\n",
    "                       1 \\\\ -\\beta\n",
    "                   \\end{bmatrix}\n",
    "                   \\begin{bmatrix}\n",
    "                       1 \\\\ -\\beta\n",
    "                   \\end{bmatrix}^{\\intercal}\n",
    "               }_{\\eta}\n",
    "               \\underbrace{\n",
    "                   \\begin{bmatrix}\n",
    "                       y_{t} \\\\ x_{t}\n",
    "                   \\end{bmatrix}\n",
    "                   \\begin{bmatrix}\n",
    "                       y_{t} \\\\ x_{t}\n",
    "                   \\end{bmatrix}^{\\intercal}\n",
    "               }_{T(y_{t}, x_{t})}    \n",
    "           \\bigg)\n",
    "       \\Bigg\\}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we need a convenient prior, preferably conjugate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Remark.: The only \"trick\" above is a matrix trace Tr. It is defined as the sum of diagonal elements, e.g., the 3x3 identity matrix has Tr(I) = 1 + 1 + 1 = 3. The trace has very appealing properties. E.g., for three compatible matrices A, B, and C it holds $Tr(ABC) = Tr(CAB) = Tr(BCA)$. From this follows this:*\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x &= [a, b], \\\\\n",
    "y &= [c, d]^{\\intercal}, \\\\\n",
    "x\\cdot y &= ac + bd = Tr(x\\cdot y) \\qquad\\text{(trace of a scalar is the same scalar)} \\\\\n",
    "Tr(y\\cdot x) &= Tr([c, d]^{\\intercal} [a, b]) \\\\ \n",
    "&= Tr\n",
    "\\begin{bmatrix}\n",
    "ac & ad \\\\\n",
    "bc & bd\n",
    "\\end{bmatrix}\n",
    "= ac + bd. \\qquad\\text{(trace preserves the result under rotation of arguments)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior distribution\n",
    "Since we do not know neigther the regression coefficients $\\beta$ nor the noise variance $\\sigma^2$, we aim to estimate both. It is known that a convenient prior distribution $\\pi(\\beta, \\sigma^2)$ is the **normal inverse-gamma** distribution with a pdf\n",
    "\n",
    "$$\n",
    "\\beta, \\sigma^{2} \n",
    "\\sim \\mathcal{N}i\\mathcal{G}(m_{t-1}, V_{t-1}, a_{t-1}, b_{t-1})\n",
    "= \\underbrace{\\mathcal{N}(m_{t-1}, \\sigma^{2} V_{t-1})}_{\\pi(\\beta|\\sigma^2)} \n",
    "\\times \n",
    "\\underbrace{i\\mathcal{G}(a_{t-1}, b_{t-1})}_{\\pi(\\sigma^2)},\n",
    "$$\n",
    "\n",
    "with real hyperparameters $a_{t-1}>0$ a $b_{t-1}>0$, vector of mean values $m_{t-1}\\in\\mathbb{R}^{p}$ and a scale matrix $V_{t-1}^{-1}$ of a corresponding size. The figure depicts examples of the marginal normal and inverse-gamma distributions. Naturally, the NiG distribution is more complicated.\n",
    "![N x iG](img/l2-apriorno-nig.jpg)\n",
    "\n",
    "The curious students may be want to see the pdf. It is as follows:\n",
    "\n",
    "$$\n",
    "\\pi(\\beta, \\sigma^{2}|\\cdot)\n",
    "    = \\frac{b^{a_{t-1}} (\\sigma^{2})^{-(a_{t-1}+1+\\frac{p}{2})}}{\\sqrt{2\\pi}|V_{t-1}|^{\\frac{1}{2}}\\Gamma(a_{t-1})}\n",
    "       \\exp\n",
    "       \\Bigg\\{ \n",
    "           -\\frac{1}{2\\sigma^{2}}\n",
    "           \\bigg[ \n",
    "           b_{t-1} + \n",
    "               \\text{Tr}\n",
    "               \\bigg( \n",
    "                       \\begin{bmatrix}\n",
    "                           1 \\\\ -\\beta\n",
    "                       \\end{bmatrix}^{\\intercal}\n",
    "                       \\begin{bmatrix}\n",
    "                           1 \\\\ -\\beta\n",
    "                       \\end{bmatrix}\n",
    "                       \\begin{bmatrix}\n",
    "                           m_{t-1}^{\\intercal} \\\\ I \n",
    "                       \\end{bmatrix}\n",
    "                       V_{t-1}^{-1}\n",
    "                       \\begin{bmatrix}\n",
    "                           m_{t-1}^{\\intercal} \\\\ I \n",
    "                       \\end{bmatrix}^{\\intercal}\n",
    "                \\bigg)\n",
    "            \\bigg]\n",
    "       \\Bigg\\}.\n",
    "$$\n",
    "\n",
    "If we look at the model, we surely want to derive $\\xi_{t-1}$ a $\\nu_{t-1}$ that will replace (or more precisely *represent*) our $a_{t-1}, b_{t-1}, m_{t-1}$, and $V_{t-1}$. These are:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\xi_{t-1} \n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "        m_{t-1}^{\\intercal} V_{t-1}^{-1} m_{t-1} + 2b_{t-1} & m_{t-1}^{\\intercal} V_{t-1}^{-1} \\\\\n",
    "        V_{t-1}^{-1}m_{t-1} & V_{t-1}^{-1}\n",
    "    \\end{bmatrix} \\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "        \\xi_{t-1}^{[11]} & \\xi_{t-1}^{[12]} \\\\\n",
    "        \\xi_{t-1}^{[21]} & \\xi_{t-1}^{[22]}\n",
    "    \\end{bmatrix}, \\\\\n",
    "\\nu_{t-1} &= 2a_{t-1}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now, the Bayes' theorem can be used in its full simplicity due to the conjugate representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian update\n",
    "Recall that the update is a sum of $\\xi_{t-1}$ and $T(y_t, x_t)$. Simple algebra shows that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    V_{t} &= \\left( V_{t-1}^{-1} + x_{t}x_{t}^{\\intercal} \\right)^{-1}\n",
    "           = V_{t-1} - \\frac{V_{t-1} x_{t}x_{t}^{\\intercal} V_{t-1}}{1+x_{t}^{\\intercal} V_{t-1} x_{t}}= \\left(\\xi_{t}^{[22]}\\right)^{-1}, \\\\ \n",
    "    m_{t} &= V_{t}(V_{t-1}^{-1}m_{t-1} + y_{t}x_{t}) = \\left(\\xi_{t}^{[22]}\\right)^{-1} \\xi_{t}^{[21]}, \\\\\n",
    "    a_{t} &= a_{t-1} + \\frac{1}{2} = \\frac{1}{2}(\\nu_{t-1} + 1) = \\frac{1}{2}\\nu_{t}, \\label{eq:nig-update} \\\\\n",
    "    b_{t} &= b_{t-1} + \\frac{1}{2} \\left(-m_{t}^{\\intercal}V_{t}^{-1}m_{t} + m_{t-1}^{\\intercal} V_{t-1}^{-1}m_{t-1} + y_{t}^{2} \\right) \\\\\n",
    "    &= \\frac{1}{2}\\left[\\xi_{t}^{[11]} - \\xi_{t}^{[12]}\\left( \\xi_{t}^{[22]} \\right)^{-1} \\left( \\xi_{t}^{[12]} \\right)^{\\intercal}\\right], \\notag\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "An interesting point is that this approach to the derivation of the posterior hyperparameters is easy. However, the standard literature mostly uses the traditional forms of pdfs and a tedious algebra with certain tricks ;-)\n",
    "\n",
    "Recall that\n",
    "\n",
    "$$\n",
    "\\beta, \\sigma^{2} = \n",
    "\\underbrace{\\mathcal{N}(m_{t-1}, \\sigma^{2} V_{t-1})}_{\\pi(\\beta|\\sigma^2)} \n",
    "\\times\n",
    "\\underbrace{i\\mathcal{G}(a_{t-1}, b_{t-1})}_{\\pi(\\sigma^2)}.\n",
    "$$\n",
    "\n",
    "**The estimates follow from the marginal distributions:**\n",
    "- $\\hat{\\sigma}^2 = \\frac{b_{t}}{a_{t}-1}$. This follows from the marginal [inverse-gamma distribution](https://en.wikipedia.org/wiki/Inverse-gamma_distribution). The variance is $\\operatorname{var}(\\sigma^{2}|\\cdot) = \\frac{b_{t}^{2}}{(a_{t}-1)^{2}(a_{t}-2)}$. **The uncertainty - measured by variance - tends to zero with  $t\\to\\infty$.**\n",
    "- $\\hat{\\beta} = m_t$ follows from the marginal distribution $\\int \\pi(\\beta|\\sigma^2) \\pi(\\sigma^2)d\\sigma^2$ which is the [Student t distribuion](https://en.wikipedia.org/wiki/Student%27s_t-distribution#Non-standardized_Student%27s_t-distribution) with $2a_t$ degrees of freedom, centered in $m_t$ and with a scale matrix $\\frac{b_t}{a_t}V_t$. The variance $var \\beta = \\frac{b_t}{a_t-1}V_t$. **The uncertainty follows from the finite number of measurements and from the noise $\\varepsilon_t$ with the variance $\\sigma^2$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The regression with the altitude-model:\n",
    "> - estimate of $\\beta = [\\beta_1, \\beta_2] \\equiv [v_0, a]$ including the $\\pm$3 standard deviations band\n",
    "![Ebeta](img/l2-regrese-Ebeta.jpg)\n",
    "> - detail\n",
    "![Ebeta detail](img/l2-regrese-Ebeta-detail.jpg)\n",
    "> - estimate of $\\sigma^2$ including $\\pm$3 standard deviations band,\n",
    "![Esigma2](img/l2-regrese-Esigma2.jpg)\n",
    "\n",
    "> **Naturally it depends on the prior distribution. If it were too narrow and with a wrong location, the convergence would be slower. On the other hand, a flat prior converges faster, but the uncertainty (variance) will be larger in the beginning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Assume that we want to know (predict) the value of $y'$ for given $x'$, e.g., the future value. The Bayesian approach performs predictions via the *predictive distribution*, which reads:\n",
    "\n",
    "$$\n",
    "f(y'|y_{0:t},x_{0:t},x') = \\iint f(y'|x', \\beta, \\sigma^{2}) \\pi(\\beta, \\sigma|y_{0:t}, x_{0:t}) \\mathrm{d}\\beta \\mathrm{d}\\sigma^{2}.\n",
    "$$\n",
    "\n",
    "This is again the [Student t distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution#Non-standardized_Student%27s_t-distribution)\n",
    "$$\n",
    "y'|y_{0:t}, x_{0:t}, x' \\sim t_{2a_{t}}\\left(m_{t}^{\\intercal}x', \\frac{b_{t}}{a_{t}} \\left(1 + (x')^{\\intercal}V_{t}x'\\right) \\right).\n",
    "$$\n",
    "\n",
    "Note that it is centered at $m_t^{\\intercal} x' = \\hat{\\beta}^\\intercal x'$, exactly as axpected. Moreover, we have a measure of uncertainty - $var(y'|\\cdot) = \\frac{b_t}{a_t-1} \\left(1 + (x')^{\\intercal}V_{t}x'\\right)$. **This combines the uncertainty both in $\\beta$ and $\\sigma^2$.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
