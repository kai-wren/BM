{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Generalized Linear Models (GLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation examples\n",
    "\n",
    "\"Classical\" linear models, although very useful, are not suitable for many different problems. For instance, if the modelled variable is defined on [0, 1], or it is binary {0, 1}, or when it denotes counts. Some examples:\n",
    "- number of customers in eshop based on season, sales, ranking, advertisement etc.\n",
    "- result of cancer treatment based on different medication and patient's predispositions.\n",
    "- result of admission tests (passed-failed) based on previous study results and other factors.\n",
    "- presence of an infection after ceasean section in dependence of its planned indication.\n",
    "- number of deaths due to respiratory diseases caused by air pollution.\n",
    "- number of high-energetic particles incident to a satellite instrument based on the sun activity, eruptions and satellite position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLMs - a bit of math\n",
    "\n",
    "Similarly to the *linear models* we denote the observed variable as $y_t$. Furthemore we assume that it is distributed according to a **distribution from the EF**, i.e., normal, binomial, Poisson, gamma... Again, we assume an independent **explanatory variable - regressor** $x_t$, and a **vector of regression coefficients** $\\beta$ of the same size as $x_t$.\n",
    "\n",
    "Recall the linear regression, where $y_t = \\beta^\\intercal x_t + \\varepsilon_t$.\n",
    "\n",
    "> **Generalized linear models** are models of the form\n",
    "\n",
    ">$$\n",
    "\\widehat{y}_t = \\mathbb{E}[y_t|x_t, \\beta] = g^{-1}(\\beta^\\intercal x_t),\n",
    "$$\n",
    "\n",
    "> where $\\widehat{y}_t = \\mathbb{E}[y_t|x_t, \\beta]$ is the expected value of $y_t$, $\\beta^\\intercal x_t$ is the linear predictor as in the linear models, and $g$ is the link function.\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\n",
    "\\beta^\\intercal x_t = g(\\mathbb{E}[y_t|x_t, \\beta]) = g(\\widehat{y}_t),\n",
    "$$\n",
    "\n",
    "i.e., the link function links the linear predictor $\\beta^\\intercal x_t$ with the expected value of $y_t$.\n",
    "\n",
    "**Think abouts...**\n",
    "- the linear regression is a GLM too. What are $g$ a $g^{-1}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of link functions $g$\n",
    "#### Binary variable $y \\sim \\{0, 1\\}$\n",
    "For binary Bernoulli-distributed variables with the probability $p$ we often use the [logit function](https://en.wikipedia.org/wiki/Logit):\n",
    "\n",
    "$$\n",
    "\\mathit{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right).\n",
    "$$\n",
    "\n",
    "Its inverse is called the [logistic function](https://en.wikipedia.org/wiki/Logistic_function), aka sigmoid function (but sigmod functions is a whole [class of functions](https://en.wikipedia.org/wiki/Logit)). Logistic function has the form\n",
    "\n",
    "$$\n",
    "\\mathit{logit}^{-1}(z) = \\sigma(z) = \\frac{e^z}{1+e^z} = \\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "\n",
    "In place of $z$ we use $\\beta^\\intercal x_t$ which yields the **logistic regression**. The plots show both functions.\n",
    "\n",
    "![logit](img/logit_expit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poisson variable $y \\sim \\textit{Poisson}(\\cdot)$\n",
    "\n",
    "The Poisson distribution is used for the description of the number of events occuring in during a certain time span or at a certain place, e.g., the number of particles incident to a detector between two time instants, or the number of customers contacting a call center in dependence to time, the number of deaths in a given age group during a time period etc. Here, we need to project the linear predictor $\\beta^\\intercal x_t$ to a set of positive numbers. The logithm function is convenient for this. The inverse function is naturally the exponential function.\n",
    "\n",
    "The **Poisson regression** uses the following model:\n",
    "\n",
    "$$\n",
    "\\log \\mathbb{E}[y|x, \\beta] = \\beta^\\intercal x, \\qquad \\text{or}\\qquad \\mathbb{E}[y|x, \\beta] = e^{\\beta^\\intercal x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression serves for modelling binary random variable\n",
    "\n",
    "$$\n",
    "y_t = \n",
    "\\begin{cases} \n",
    "0 \\quad \\text{with probability $p_t$}, \\\\ \n",
    "1 \\quad \\text{with probability $1 - p_t$}.\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We already know that a suitable distribution for $y_t$ is the **Bernoulli distribution** parameterized by $p_t$,\n",
    "\n",
    "$$\n",
    "y_t \\sim \\textit{Bernoulli}(p_t)\n",
    "$$\n",
    "and with the **mean value** $\\mathbb{E}[y_t] = p_t$.\n",
    "\n",
    "If there is a suitable explanatory variable - regressor $x_t$, we can try to estimate $\\beta$ and $y_t$ link with the linear predictor.\n",
    "\n",
    "Recall, that the pmf of the Bernoulli distribution is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(y_t|p_t) &= p_t^{y_t} (1-p_t)^{1-y_t} \\\\\n",
    "           &= f(y_t|x_t, \\beta),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where the second line will become clear shortly :-)\n",
    "\n",
    "From the theory of GLMs we know that we need a suitable link function that links the *linear predictor* $\\beta^\\intercal x_t$ with $y_t$, more precisely with its expected value,\n",
    "\n",
    "$$\n",
    "\\widehat{y}_t = \\mathbb{E}[y_t|x_t, \\beta] = g^{-1}(\\beta^\\intercal x_t).\n",
    "$$\n",
    "\n",
    "Moreover, we already know that such a $g$ is the $\\textit{logit}$ function, hence\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g(\\mathbb{E}[y_t|p_t]) \n",
    "&= g(\\mathbb{E}[y_t|X_t, \\beta]) \\\\\n",
    "&= g(p_t) \\\\\n",
    "&= \\textit{logit}(p_t) \\\\\n",
    "&= \\log \\frac{p_t}{1-p_t}\n",
    "= \\beta^\\intercal x_t. \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Analogously, by inversion\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{y}_t = \\mathbb{E}[y_t|x_t, \\beta]\n",
    "&= p_t \\\\\n",
    "&= \\textit{logit}^{-1}(\\beta^\\intercal x_t) \\\\\n",
    "&= \\sigma(\\beta^\\intercal x_t) \\\\\n",
    "&= \\frac{1}{1+e^{-\\beta^\\intercal x_t}}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian estimation of logistic regression model\n",
    "\n",
    "Unfortunately, Bayesian estimation of $\\beta$ is a non-trivial task, since the distribution\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\pi(\\beta|x_{0:t}, y_{0:t}) \n",
    "&\\propto f(y_t|x_t, \\beta) \\times \\pi(\\beta|x_{0:t-1}, y_{0:t-1}) \\\\\n",
    "&\\propto p_t^{y_t} (1-p_t)^{1-y_t}) \\times \\pi(\\beta|x_{0:t-1}, y_{0:t-1}) \\\\\n",
    "&\\propto \\widehat{y}_t^{y_t}\n",
    "(1-\\widehat{y}_t)^{1-y_t} \\times \\pi(\\beta|x_{0:t-1}, y_{0:t-1})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "is not analytically reachable for any reasonable prior distribution. The alternative ways consist in the use of Monte Carlo approximations, variational approximations or Laplace approximations. Let us focus on the last one for its straightforward use in sequential (online) modelling.\n",
    "\n",
    "We start with the normal prior distribution,\n",
    "\n",
    "$$\n",
    "\\beta|x_{0:t-1}, y_{0:t-1} \\sim \\mathcal{N}(\\widehat{\\beta}_{t-1}, \\Sigma_{t-1}),\n",
    "$$\n",
    "\n",
    "and try to preserve (approximate) normality after the Bayes update.\n",
    "\n",
    "The posterior distribution is found in two steps:\n",
    "1. MAP (maximum a posterior) estimation of mean value - we find the maximum of the posterior. This will be the location of the mean of the approximating normal posterior distribution.\n",
    "2. We find a suitable covariance matrix.\n",
    "\n",
    "**Think abouts...**\n",
    "- generally: how do we find a maximum of a function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Intermezzo: Newton's method**\n",
    ">\n",
    "> If there is \"nice\" function $g$ - namely continuous with existing derivative and monotonous in a studied interval / then finding the point $x$ where  $g(x)=0$ can be done as follows:\n",
    "1. We choose a \"reasonable\" point $x_0$.\n",
    "2. We find a new point $x_1$ on the intersection of the tangent of $g(x_0)$ with the axis $x$.\n",
    "3. We repeat the two steps until reaching a necessary precision or a preset number of iterations.\n",
    "\n",
    "![Newtonova metoda](img/Newton-en.png)\n",
    "\n",
    "**Think abouts...**\n",
    "- We already know that we will seek for the maximum of the posterior $\\pi(\\beta|x_{0:t}, y_{0:t})$. What is the analogy of the function $g$ in the \"Intermezzo\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching the mean value of the approximating normal distribution\n",
    "\n",
    "We will exploit the Newton's method for searching the maximum $\\widehat{\\beta}_t$. In the literature, a single-step Newton's method was used, where the initial point (analogous to $x_0$ in \"Intermezzo\") the previous mean $\\widehat{\\beta}_{t-1}$ was chosen,\n",
    "\n",
    "$$\n",
    "\\widehat{\\beta}_t = \\widehat{\\beta}_{t-1} - [l''(\\widehat{\\beta}_{t-1})]^{-1} l'(\\widehat{\\beta}_{t-1}),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "l(\\widehat{\\beta}_{t-1}) = \\log \\pi(\\beta|x_{0:t}, y_{0:t}) \n",
    "$$\n",
    "\n",
    "is the logarithm of the posterior at the point $\\widehat{\\beta}_{t-1}$. For completeness,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "l'(\\widehat{\\beta}_{t-1}) \n",
    "&= \\left(y_t - \\widehat{y}_t \\right)x_t,\\\\\n",
    "l''(\\widehat{\\beta}_{t-1})\n",
    "&= -\\Sigma_{t-1} - \\widehat{y}_t(1-\\widehat{y}_t) x_t x_t^\\intercal.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching the covariance of the approximating normal distribution\n",
    "\n",
    "If we have the MAP estimate, we fit the covariance matrix as\n",
    "$$\n",
    "\\Sigma_t = -[l''(\\widehat{\\beta}_{t-1})]^{-1}.\n",
    "$$\n",
    "\n",
    "Note that we already have this :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "**Inicialization**\n",
    "- Set the initial normal hyperparameters $\\widehat\\beta_{t-1}, \\Sigma_{t-1}$.\n",
    "\n",
    "**Online modelling**\n",
    "1. Acquire new $y_t$ and $x_t$.\n",
    "2. Calculate MAP estimate $\\widehat{\\beta}_t$.\n",
    "3. Calculate approximate covariance $\\Sigma_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "We already know that the Bayesian prediction needs the predictive distribution. However, similarly to the true posterior distribution, it is not available here, but we may use the Laplacian approximation,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(y'|x', x_{0:t}, y_{0:t}) \n",
    "&= \\int f(y'|x', x_{0:t}, y_{0:t}, \\beta) \\pi(\\beta|x_{0:t}, y_{0:t}) d\\beta  \\\\\n",
    "&\\approx\n",
    "(2\\pi)^{\\frac{n}{2}} \\left[\\det l''(\\widehat{\\beta}_t)\\right]^{-\\frac{1}{2}} f(y'|x', \\beta)\\ \\pi(\\beta|x_{0:t},y_{0:t}),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where we set $\\beta = \\widehat{\\beta}_t$, and where $n$ is the size of $\\beta$.\n",
    "\n",
    "#### Prediction - remark\n",
    "\n",
    "It should be stressed that the prediction - Bayesian or not - yields a real number from [0, 1], while the modelled variable is binary, i.e., either 0 or 1. Quite naturally we use a threshold $m \\in [0, 1]$, mostly 0.5, and predict\n",
    "\n",
    "$$\n",
    "\\widehat{y} =\n",
    "\\begin{cases}\n",
    "0 \\Leftrightarrow y' < m, \\\\\n",
    "1 \\Leftrightarrow y' \\geq m.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Brier score\n",
    "The [Brier score](https://en.wikipedia.org/wiki/Brier_score) measures the prediction quality\n",
    "$$\n",
    "B = \\frac{1}{T} \\sum_{t=1}^{T} (y_t' - y_t)^2,\n",
    "$$\n",
    "\n",
    "where $y_t'$ may be either from [0, 1], or {0, 1}, which should be indicated.\n",
    "Brier score is the equivalent of the mean squared error [MSE](https://en.wikipedia.org/wiki/Mean_squared_error).\n",
    "\n",
    "\n",
    "### Confusion matrix (table)\n",
    "[Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix#Table_of_confusion) is a type of the [contingency table](https://en.wikipedia.org/wiki/Contingency_table) used in classification. For instance, in spam classification, it may look as follows:\n",
    "\n",
    "| | Spam | Non-Spam |\n",
    "| --- | --- | --- |\n",
    "|** Pred. spam ** | 100 | 2 |\n",
    "|** Pred. non-spam ** | 10 | 500 |\n",
    "\n",
    "The matrix can be used for the calculation of a bunch of diagnostic values, e.g., the [sensitivity and specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity), [power](https://en.wikipedia.org/wiki/Statistical_power), [Type I and II errors](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors), [DOR](https://en.wikipedia.org/wiki/Diagnostic_odds_ratio) etc.\n",
    "![confusion_matrix](img/confusion_matrix.png)\n",
    "See: [https://en.wikipedia.org/wiki/Confusion_matrix#Table_of_confusion]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin - NonSkin classification\n",
    "\n",
    "The example considers the Skin-NonSkin dataset of Bhatt and Dhall. It consists of 245,057 samples of which 50,859 are skin samples and 194,198 are non-skin samples. The dataset was collected by randomly sampling RGB values from face images of various age groups (young, middle, and old), race groups (white, black, and Asian), and gender. The samples were obtained from the FERET and PAL databases. Each data item consisted of four variables -- B, G, R and the class label.\n",
    "\n",
    "Our goal is to estimate the logistic model parameters where the regression vectors were $x_{t} = [1, B_{t}, G_{t}, R_{t}]^{\\intercal}$ (the first term standing for the offset) and the dependent variable $y_{t}$ denotes the class (1 is Skin, 2 is NonSkin - hence we need to subtract 1). The data were randomly shuffled before processing  and were introduced sequentially. We use a subset of 1000 randomly chosen samples and run their sequential classification. The normal prior was $\\mathcal{N}$(**_0_**, 100**_I_**), and the threshold was _m_=0.5.\n",
    "\n",
    "\n",
    "Three randomly chosen rows:\n",
    "\n",
    "    ---\n",
    "    B, G, R, Class\n",
    "    242, 169, 161,   2\n",
    "    218, 211, 202,   2\n",
    "    110, 150, 209,   1\n",
    "    ---\n",
    "\n",
    "## Results:\n",
    "\n",
    "| | Skin | NonSkin | Sum |\n",
    "|---|---|---|---|\n",
    "|Pred. Skin| 188 | 42 | 230 |\n",
    "|Pred. NonSkin| 51 | 719 | 770 |\n",
    "| Sum | 239 | 761 | 1000|\n",
    "\n",
    "\n",
    "    TPR: 0.933766233766\n",
    "    TNR: 0.817391304348\n",
    "    PPV: 0.944809461235\n",
    "    NPV: 0.786610878661\n",
    "    FPR: 0.182608695652\n",
    "    FDR: 0.0551905387648\n",
    "    FNR: 0.0662337662338\n",
    "    ACC: 0.907\n",
    "    F1_score: 0.939255388635\n",
    "    informedness = TPR+TNR-1: 0.751157538114\n",
    "    markedness = PPV+NPV-1: 0.731420339896\n",
    "    prevalence: 0.77\n",
    "    LRP: 5.11348175634\n",
    "    LRN: 0.0810306714562\n",
    "    DOR: 63.1055088702\n",
    "    FOR: 0.213389121339\n",
    "\n",
    "\n",
    "Brier score evolution (final value 0.085):\n",
    "![Brier](img/l5-brier.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
